{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrianstop/arduino-magic-wand/blob/main/magic_wand_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f92-4Hjy7kA8"
      },
      "source": [
        "# Gesture Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvDA8AK7QOq-"
      },
      "source": [
        "## Setup Python Environment \n",
        "\n",
        "The next cell sets up the dependencies required for the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2gs-PL4xDkZ"
      },
      "outputs": [],
      "source": [
        "# Setup environment\n",
        "!apt-get -qq install xxd\n",
        "!pip install pandas numpy matplotlib\n",
        "!pip install tensorflow\n",
        "!pip install tinymlgen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lwkeshJk7dg"
      },
      "source": [
        "## Get Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXzdey6thBCE"
      },
      "outputs": [],
      "source": [
        "# 30 sample of each dataset\n",
        "#!wget https://github.com/adrianstop/arduino-magic-wand/raw/main/training_data/training_data.tar\n",
        "#!tar xf training_data.tar\n",
        "#!rm -rf training_data.tar\n",
        "\n",
        "# 50 sample of each dataset\n",
        "!wget https://github.com/adrianstop/arduino-magic-wand/raw/main/training_data/training_data_large.tar\n",
        "!tar xf training_data_large.tar\n",
        "!rm -rf training_data_large.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh9yve14gUyD"
      },
      "source": [
        "## Plot Data\n",
        "\n",
        "We'll graph the input files on two separate graphs, acceleration and gyroscope, as each data set has different units and scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I65ukChEgyNp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# filename = \"alohomora.csv\"\n",
        "# filename = \"arresto_momentum.csv\"\n",
        "# filename = \"avadakedavra.csv\"\n",
        "filename = \"locomotor.csv\"\n",
        "#filename = \"revelio.csv\"\n",
        "\n",
        "df = pd.read_csv(filename)\n",
        "\n",
        "index = range(1, len(df['aX']) + 1)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,4)\n",
        "\n",
        "plt.plot(index, df['aX'], 'g.', label='x', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['aY'], 'b.', label='y', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['aZ'], 'r.', label='z', linestyle='solid', marker=',')\n",
        "plt.title(\"Acceleration\")\n",
        "plt.xlabel(\"Sample #\")\n",
        "plt.ylabel(\"Acceleration (G)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(index, df['gX'], 'g.', label='x', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['gY'], 'b.', label='y', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['gZ'], 'r.', label='z', linestyle='solid', marker=',')\n",
        "plt.title(\"Gyroscope\")\n",
        "plt.xlabel(\"Sample #\")\n",
        "plt.ylabel(\"Gyroscope (deg/sec)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(index, df['mX'], 'g.', label='x', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['mY'], 'b.', label='y', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['mZ'], 'r.', label='z', linestyle='solid', marker=',')\n",
        "plt.title(\"Magnetometer\")\n",
        "plt.xlabel(\"Sample #\")\n",
        "plt.ylabel(\"Magnetic field strength (uT)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk414PU3oy3"
      },
      "source": [
        "## Parse and prepare the data\n",
        "\n",
        "The next cell parses the csv files and transforms them to a format that will be used to train the fully connected neural network.\n",
        "\n",
        "Update the `FORMULAS` list with the gesture data you've collected in `.csv` format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGChd1FAk5_j"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
        "\n",
        "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
        "# the same random numbers each time the notebook is run\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# the list of formulas that data is available for\n",
        "FORMULAS = [\n",
        "    \"alohomora\",\n",
        "    \"arresto_momentum\",\n",
        "    \"avadakedavra\",\n",
        "    \"locomotor\",\n",
        "    \"revelio\"  \n",
        "]\n",
        "\n",
        "SAMPLES_PER_FORMULA = 119\n",
        "AVERAGE_NUM_SAMPLES = 1\n",
        "NUM_FORMULAS = len(FORMULAS)\n",
        "\n",
        "# create a one-hot encoded matrix that is used in the output\n",
        "ONE_HOT_ENCODED_FORMULAS = np.eye(NUM_FORMULAS)\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# read each csv file and push an input and output\n",
        "for formula_index in range(NUM_FORMULAS):\n",
        "  formula = FORMULAS[formula_index]\n",
        "  print(f\"Processing index {formula_index} for formula '{formula}'.\")\n",
        "  \n",
        "  output = ONE_HOT_ENCODED_FORMULAS[formula_index]\n",
        "  \n",
        "  df = pd.read_csv(formula + \".csv\")\n",
        "  \n",
        "  # calculate the number of gesture recordings in the file\n",
        "  num_recordings = int(df.shape[0] / SAMPLES_PER_FORMULA)\n",
        "  \n",
        "  print(f\"\\tThere are {num_recordings} recordings of the {formula} formula.\")\n",
        "  \n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    datapoint = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    for j in range(SAMPLES_PER_FORMULA):\n",
        "      index = i * SAMPLES_PER_FORMULA + j\n",
        "      # normalize the input data, between 0 to 1:\n",
        "      # - acceleration is between: -4 to +4\n",
        "      # - gyroscope is between: -2000 to +2000\n",
        "      # - magnetometer is between: -400 to +400\n",
        "      datapoint = [\n",
        "          ((df['aX'][index] + 4) / 8)/AVERAGE_NUM_SAMPLES + datapoint[0],\n",
        "          ((df['aY'][index] + 4) / 8)/AVERAGE_NUM_SAMPLES + datapoint[1],\n",
        "          ((df['aZ'][index] + 4) / 8)/AVERAGE_NUM_SAMPLES + datapoint[2],\n",
        "          ((df['gX'][index] + 2000) / 4000)/AVERAGE_NUM_SAMPLES + datapoint[3],\n",
        "          ((df['gY'][index] + 2000) / 4000)/AVERAGE_NUM_SAMPLES + datapoint[4],\n",
        "          ((df['gZ'][index] + 2000) / 4000)/AVERAGE_NUM_SAMPLES + datapoint[5],\n",
        "          ((df['mX'][index] + 400) / 800)/AVERAGE_NUM_SAMPLES + datapoint[6],\n",
        "          ((df['mY'][index] + 400) / 800)/AVERAGE_NUM_SAMPLES + datapoint[7],\n",
        "          ((df['mZ'][index] + 400) / 800)/AVERAGE_NUM_SAMPLES + datapoint[8]\n",
        "      ]\n",
        "      if(j%AVERAGE_NUM_SAMPLES == 0):\n",
        "        tensor += datapoint\n",
        "        datapoint = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "      \n",
        "    #tensor_t = list(map(list, zip(*tensor))) # transpose data\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "# convert the list to numpy array\n",
        "inputs = np.array(inputs, dtype='float32')\n",
        "outputs = np.array(outputs, dtype='float32')\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")\n",
        "#print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "outputs_n = []\n",
        "for i in range(outputs.shape[0]):\n",
        "  for j in range(outputs.shape[1]):    \n",
        "    if(outputs[i][j] == 1):\n",
        "      outputs_n.append(j)\n",
        "print(outputs_n)\n",
        "\n",
        "K = 200\n",
        "selector = SelectKBest(f_classif, k=K)\n",
        "selected_features = selector.fit_transform(inputs, outputs_n)\n",
        "f_score_indexes = (-selector.scores_).argsort()[:K]\n",
        "print(f_score_indexes)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "plt.plot(selector.scores_)\n",
        "plt.title('F-score')\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('Importance')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PEImlF5JE0JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5_61831d5AM"
      },
      "source": [
        "## Randomize and split the input and output pairs for training\n",
        "\n",
        "Randomly split input and output pairs into sets of data: 60% for training, 20% for validation, and 20% for testing.\n",
        "\n",
        "  - the training set is used to train the model\n",
        "  - the validation set is used to measure how well the model is performing during training\n",
        "  - the testing set is used to test the model after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfNEmUZMeIEx"
      },
      "outputs": [],
      "source": [
        "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
        "# https://stackoverflow.com/a/37710486/2020087\n",
        "num_inputs = len(inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
        "inputs = inputs[randomize]\n",
        "inputs = inputs.reshape(inputs.shape[0], 1, inputs.shape[1])\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
        "TRAIN_SPLIT = int(0.7 * num_inputs)\n",
        "TEST_SPLIT = int(0.1 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "print(inputs_train.shape)\n",
        "print(\"Data set randomization and splitting complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9g2n41p24nR"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGNFa-lX24Qo"
      },
      "outputs": [],
      "source": [
        "# build the model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(inputs_train.shape[1], inputs_train.shape[2])),\n",
        "    tf.keras.layers.LSTM(30, unroll=False, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(NUM_FORMULAS, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwm1PdqXPnca"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "num_epochs = 500\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=100, restore_best_weights=True)\n",
        "lr_values = [1e-3, 1e-4]\n",
        "lr_boundaries = [num_inputs/2]\n",
        "lr_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(lr_boundaries, lr_values)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_fn)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['mae', 'accuracy'])\n",
        "\n",
        "# Train the model \n",
        "history = model.fit(inputs_train, outputs_train, epochs=num_epochs, batch_size=1, validation_data=(inputs_validate, outputs_validate), callbacks=[es])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTzUSG6uyFAF"
      },
      "source": [
        "## Measure FLOPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dUr_dUnyMPk"
      },
      "outputs": [],
      "source": [
        "pip install keras-flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRPHKHtYyL_6"
      },
      "outputs": [],
      "source": [
        "# get_flops not supported for LSTM network\n",
        "\n",
        "#from keras_flops import get_flops\n",
        "\n",
        "#flops = get_flops(model, batch_size=1)\n",
        "#print(f\"FLOPS: {flops / 10 ** 3:.03} K\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUDPvaJE1wRE"
      },
      "source": [
        "## Verify \n",
        "\n",
        "Graph the models performance vs validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMFdf1_a1rU4"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxA0zCOaS35v"
      },
      "source": [
        "### Graph the loss\n",
        "\n",
        "Graph the loss to see when the model stops improving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvFNHXoQzmcM"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "\n",
        "# graph the loss, the model above is configure to use \"mean squared error\" as the loss function\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'g-', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(plt.rcParams[\"figure.figsize\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRjvkFQy2RgS"
      },
      "source": [
        "### Graph the mean absolute error\n",
        "\n",
        "[Mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) is another metric to judge the performance of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBjCf1-2zx9C"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "\n",
        "# graph of mean absolute error\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "plt.plot(epochs[:], mae[:], 'g-', label='Training MAE')\n",
        "plt.plot(epochs[:], val_mae[:], 'b-', label='Validation MAE')\n",
        "plt.title('Training and validation mean absolute error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMjtfa42ahM"
      },
      "source": [
        "### Run with Test Data\n",
        "Put our test data into the model and plot the predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3Y0CCWJz2EK"
      },
      "outputs": [],
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(inputs_test)\n",
        "predictions_round = np.round(predictions, decimals=0)\n",
        "\n",
        "# print the predictions and the expected ouputs\n",
        "print(\"predictions =\\n\", predictions_round)\n",
        "print(\"actual =\\n\", outputs_test)\n",
        "\n",
        "mistake_counter = 0\n",
        "for i in range(len(predictions_round)):\n",
        "  for j in range(len(predictions_round[1])):\n",
        "    if predictions_round[i][j] != outputs_test[i][j]:\n",
        "      mistake_counter += 1\n",
        "      break\n",
        "\n",
        "print(\"Correct predictions: {0} out of {1} = {2} %\".format((len(outputs_test) - mistake_counter), len(outputs_test), 100*((len(outputs_test) - mistake_counter)/len(outputs_test))))\n",
        "\n",
        "inputs_test_2d = inputs_test.reshape(inputs_test.shape[0], inputs_test.shape[2])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "outputs_test_n = []\n",
        "predictions_n = []\n",
        "for i in range(outputs_test.shape[0]):\n",
        "  for j in range(outputs_test.shape[1]):    \n",
        "    if(outputs_test[i][j] == 1):\n",
        "      outputs_test_n.append(j)\n",
        "    if(predictions_round[i][j] == 1):\n",
        "      predictions_n.append(j)\n",
        "\n",
        "cfm = confusion_matrix(outputs_test_n, predictions_n)\n",
        "\n",
        "df_cm = pd.DataFrame(cfm, index = FORMULAS,\n",
        "                  columns = FORMULAS)\n",
        "plt.figure(figsize = (10,7))\n",
        "hm = sn.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
        "hm.set(xlabel='Predicted formula', ylabel='Actual formula', title='Test set confusion matrix')"
      ],
      "metadata": {
        "id": "HRq3PZRrPqsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DO6xxXVCym"
      },
      "source": [
        "# Convert the Trained Model to Tensor Flow Lite\n",
        "\n",
        "The next cell converts the model to TFlite format. The size in bytes of the model is also printed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xn1-Rn9Cp_8"
      },
      "outputs": [],
      "source": [
        "run_model = tf.function(lambda x: model(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = 1\n",
        "STEPS = inputs_train.shape[1]\n",
        "INPUT_SIZE = inputs_train.shape[2]\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
        "\n",
        "# model directory.\n",
        "MODEL_DIR = \"keras_wand_lstm\"\n",
        "model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"wand_model.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"wand_model.tflite\")\n",
        "print(\"Model is %d bytes\" % basic_model_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykccQn7SXrUX"
      },
      "source": [
        "# Encode the Model in an Arduino Header File \n",
        "\n",
        "The next cell creates a constant byte array that contains the TFlite model. Import it as a tab with the sketch below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J33uwpNtAku"
      },
      "outputs": [],
      "source": [
        "!echo \"const unsigned char model[] = {\" > model.h\n",
        "!cat wand_model.tflite | xxd -i      >> model.h\n",
        "!echo \"};\"                              >> model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-QtpclDFNnJ"
      },
      "source": [
        "Now it's time to switch back to the tutorial instructions and run our new model on the Arduino Nano 33 BLE Sense to classify the accelerometer and gyroscope data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eSkHZaLzMId"
      },
      "source": [
        "# Links\n",
        "\n",
        "*   https://github.com/arduino/ArduinoTensorFlowLiteTutorials/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1eSkHZaLzMId"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}